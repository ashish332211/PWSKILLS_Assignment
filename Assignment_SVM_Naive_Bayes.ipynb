{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6d913e",
   "metadata": {},
   "source": [
    "Q1. What is a Support Vector Machine (SVM), and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aa470",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm that separates data into different classes by finding the optimal separating boundary, or hyperplane, that maximizes the margin between the classes. SVMs are used for both classification and regression tasks and are particularly effective for high-dimensional data and complex, non-linear problems, which they handle using kernel functions.\n",
    "\n",
    "* How SVMs Work\n",
    "\n",
    "Finding the Hyperplane: SVMs find a hyperplane (a decision boundary) that best separates different classes of data points in a multi-dimensional space. \n",
    "\n",
    "Maximizing the Margin: The \"best\" hyperplane is the one that maximizes the margin, which is the distance between the hyperplane and the nearest data points (called \"support vectors\") from each class. \n",
    "\n",
    "Support Vectors: The support vectors are the crucial data points that lie closest to the separating hyperplane and define its position and orientation. \n",
    "\n",
    "Kernel Trick for Non-Linear Data: For data that cannot be separated by a linear hyperplane, SVMs use a kernel trick. This involves mapping the data into a higher-dimensional feature space where it becomes linearly separable, allowing for the creation of complex, non-linear decision boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8c341",
   "metadata": {},
   "source": [
    "Q2. Explain the difference between Hard Margin and Soft Margin SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3061b",
   "metadata": {},
   "source": [
    "* Hard Margin SVM\n",
    "\n",
    "Perfect Separation: Enforces a strict boundary that perfectly separates the classes, resulting in zero misclassifications on the training data. \n",
    "\n",
    "Sensitivity to Outliers: If even a single data point is an outlier or falls within the margin, a hard margin SVM might fail to find a viable decision boundary or create a boundary that is overly sensitive to noise. \n",
    "\n",
    "Linearly Separable Data: Only works with datasets where the classes can be perfectly separated by a linear hyperplane. \n",
    "\n",
    "No Training Error: The goal is to achieve a margin with a 0% error rate on the training set. \n",
    "\n",
    "* Soft Margin SVM\n",
    "\n",
    "Allows for Misclassifications: Introduces the concept of slack variables to allow for some data points to be misclassified or to fall on the wrong side of the margin. \n",
    "Addresses Outliers and Noise: More robust to outliers and overlapping classes, as it doesn't demand perfect separation. \n",
    "\n",
    "Balancing Act: Involves a trade-off between maximizing the margin (which reduces the risk of overfitting) and minimizing classification errors. \n",
    "\n",
    "Regularization Parameter (C): A parameter (often denoted as C) controls the balance between maximizing the margin and minimizing the classification errors; a higher C penalizes misclassifications more heavily. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c64c0a",
   "metadata": {},
   "source": [
    "Q3. What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1484205",
   "metadata": {},
   "source": [
    "The ‚ÄúKernel Trick‚Äù is a method used in Support Vector Machines (SVMs) to convert data (that is not linearly separable) into a higher-dimensional feature space where it may be linearly separated.\n",
    "\n",
    "This technique enables the SVM to identify a hyperplane that separates the data with the maximum margin, even when the data is not linearly separable in its original space. The kernel functions are used to compute the inner product between pairs of points in the transformed feature space without explicitly computing the transformation itself. This makes it computationally efficient to deal with high dimensional feature spaces.\n",
    "\n",
    "The \"Trick\": Instead of performing the computationally expensive explicit transformation of each data point to this higher dimension, a kernel function computes the dot product between pairs of data points in this higher-dimensional space. \n",
    "\n",
    "Example: The Radial Basis Function (RBF) Kernel\n",
    "\n",
    "Definition: The RBF kernel is a type of Radial Basis Function (RBF). It calculates a similarity between two data points, which is influenced by their distance. \n",
    "\n",
    "Use Case: The RBF kernel is widely used for its ability to handle complex, non-linear data with intricate patterns. \n",
    "\n",
    "Mechanism: For a 2D dataset, the RBF kernel projects the data into a 3D space. This projection allows data points that were previously indistinguishable in 2D to be separated by a linear plane in the higher 3D space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4c380",
   "metadata": {},
   "source": [
    "Q4. What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e4525",
   "metadata": {},
   "source": [
    "* Naive Bayes Classifier\n",
    "\n",
    "The Na√Øve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks such as text classification. They use principles of probability to perform classification tasks.\n",
    "\n",
    "It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This approach is based on the assumption that the features of the input data are conditionally independent given the class, allowing the algorithm to make predictions quickly and accurately.\n",
    "\n",
    "Why is it called \"Na√Øve\"?\n",
    "\n",
    "The term \"na√Øve\" comes from the algorithm's core assumption of conditional independence among features. \n",
    "\n",
    "Unrealistic Assumption: In reality, features often have dependencies (e.g., in text classification, certain words are more likely to appear together). \n",
    "\n",
    "Simplified Calculation: This assumption simplifies the complex probability calculations required in Bayes' theorem, as the algorithm treats each feature's contribution to the class probability independently. \n",
    "\n",
    "Practical Success: Despite this oversimplified assumption, Na√Øve Bayes classifiers have proven to be effective in many real-world applications, such as spam filtering, because the independence assumption allows for efficient and accurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32439745",
   "metadata": {},
   "source": [
    "Q5. Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n",
    "When would you use each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae89b41",
   "metadata": {},
   "source": [
    "* Gaussian Na√Øve Bayes\n",
    "\n",
    "In this variant, we assume that the features are continuous values (like age, weight, temperature, income, etc.) and that each feature for a given class follows a normal (Gaussian) distribution.\n",
    "\n",
    "For example, if we are classifying whether a person is ‚Äúfit‚Äù or ‚Äúunfit‚Äù based on their height and weight, these numerical features are assumed to vary normally (bell-shaped curve) within each class.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "When features are continuous and roughly normally distributed.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Predicting whether a patient has a disease based on lab results.\n",
    "\n",
    "Classifying iris flowers using petal and sepal measurements.\n",
    "\n",
    "Sensor data analysis (like temperature, pressure, etc.).\n",
    "\n",
    "* Multinomial Na√Øve Bayes\n",
    "\n",
    "This model is used when features represent discrete counts ‚Äî such as how many times a word appears in a document.\n",
    "\n",
    "It assumes that the features follow a multinomial distribution, which models the probability of observing a particular set of event counts.\n",
    "\n",
    "It is one of the most common models used in text classification.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "When features are discrete frequency counts (not binary or continuous).\n",
    "\n",
    "It‚Äôs ideal for text and document classification where you use:\n",
    "\n",
    "Example:\n",
    "\n",
    "In spam detection, the number of times words like ‚Äúfree,‚Äù ‚Äúoffer,‚Äù or ‚Äúwin‚Äù appear are used as features.\n",
    "\n",
    "The Multinomial Na√Øve Bayes model uses these counts to determine if the email is spam or not.\n",
    "\n",
    "Bernoulli Na√Øve Bayes\n",
    "\n",
    "In this model, each feature is binary (0 or 1) ‚Äî indicating whether a feature exists or not in a sample.\n",
    "\n",
    "Instead of counting how many times a word appears, we only record whether the word is present.\n",
    "\n",
    "It follows the Bernoulli distribution, which models yes/no (true/false) outcomes.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "When features are binary (present/absent).\n",
    "\n",
    "Common in text classification tasks using binary word occurrence features.\n",
    "\n",
    "üìç Example:\n",
    "\n",
    "If we are classifying whether a movie review is positive or negative:\n",
    "\n",
    "Feature ‚Äúgood‚Äù = 1 if the word appears, 0 otherwise\n",
    "\n",
    "Feature ‚Äúbad‚Äù = 1 if it appears, 0 otherwise\n",
    "\n",
    "Bernoulli Na√Øve Bayes looks at presence/absence rather than how many times a word appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d1c2b",
   "metadata": {},
   "source": [
    "Q6. Write a Python program to:\n",
    "\n",
    "‚óè Load the Iris dataset\n",
    "\n",
    "‚óè Train an SVM Classifier with a linear kernel\n",
    "\n",
    "‚óè Print the model's accuracy and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e39e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f32f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a30f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59517bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a2cf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.3, 1.7, 0.5],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [6.3, 2.8, 5.1, 1.5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support vector\n",
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df67109",
   "metadata": {},
   "source": [
    "Q7. Write a Python program to:\n",
    "\n",
    "‚óè Load the Breast Cancer dataset\n",
    "\n",
    "‚óè Train a Gaussian Na√Øve Bayes model\n",
    "\n",
    "‚óè Print its classification report including precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cdf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e694700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fda1479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a59974",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d14b6c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a504424e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.94      0.92      0.93        63\n",
      "      benign       0.95      0.96      0.96       108\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.94      0.94      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the classification report\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0faf0",
   "metadata": {},
   "source": [
    "Q8. Write a Python program to:\n",
    "\n",
    "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
    "\n",
    "‚óè Print the best hyperparameters and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3321f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "X = wine.data \n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc8b6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b22f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "008e6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],          # Regularization parameter\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient for 'rbf'\n",
    "    'kernel': ['rbf']                # Use RBF kernel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100], &#x27;gamma&#x27;: [1, 0.1, 0.01, 0.001],\n",
       "                         &#x27;kernel&#x27;: [&#x27;rbf&#x27;]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100], &#x27;gamma&#x27;: [1, 0.1, 0.01, 0.001],\n",
       "                         &#x27;kernel&#x27;: [&#x27;rbf&#x27;]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\n",
       "                         'kernel': ['rbf']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply GridSearchCV to find the best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(svm, param_grid, refit=True, verbose=1, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f78e0d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters found by GridSearchCV:\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and best score from training\n",
    "print(\"\\nBest Hyperparameters found by GridSearchCV:\")\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e57a1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dec5b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe3616",
   "metadata": {},
   "source": [
    "Q9. Write a Python program to:\n",
    "\n",
    "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
    "sklearn.datasets.fetch_20newsgroups).\n",
    "\n",
    "‚óè Print the model's ROC-AUC score for its predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b729c393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score of the Na√Øve Bayes Classifier: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Load the text dataset (subset for faster training)\n",
    "categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X = newsgroups.data    # Text documents\n",
    "y = newsgroups.target  # Class labels\n",
    "\n",
    "# Convert text data into TF-IDF feature vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "#  Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train a Multinomial Na√Øve Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict class probabilities on the test data\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "# Compute ROC-AUC score\n",
    "# For multi-class classification, we use 'ovr' (One-vs-Rest) method\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "roc_auc = roc_auc_score(y_test_bin, y_prob, multi_class='ovr')\n",
    "\n",
    "# Print the ROC-AUC score\n",
    "print(\"ROC-AUC Score of the Na√Øve Bayes Classifier: {:.4f}\".format(roc_auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82823db2",
   "metadata": {},
   "source": [
    "Q10. Imagine you‚Äôre working as a data scientist for a company that handles email communications.\n",
    "\n",
    "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
    "\n",
    "‚óè Text with diverse vocabulary\n",
    "\n",
    "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
    "\n",
    "‚óè Some incomplete or missing data\n",
    "\n",
    "Explain the approach you would take to:\n",
    "\n",
    "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
    "\n",
    "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
    "\n",
    "‚óè Address class imbalance\n",
    "\n",
    "‚óè Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c4299",
   "metadata": {},
   "source": [
    "1. Data Preprocessing\n",
    "\n",
    "Before training any machine learning model, the first step is to prepare and clean the data.\n",
    "\n",
    "a) Handling Missing Data\n",
    "\n",
    "Emails might have missing fields like subject lines or message bodies.\n",
    "If an email has no content at all, it should be removed since it doesn‚Äôt provide useful information.\n",
    "However, if only some parts are missing (like subject), we can replace them with a placeholder such as ‚Äúunknown‚Äù or an empty string, so the dataset remains consistent.\n",
    "\n",
    "b) Text Cleaning\n",
    "\n",
    "Email content is messy ‚Äî it can include punctuation, numbers, links, and special characters.\n",
    "So we need to clean the text by:\n",
    "\n",
    "Converting all words to lowercase (so ‚ÄúFree‚Äù and ‚Äúfree‚Äù are treated the same).\n",
    "\n",
    "Removing punctuation, numbers, and extra spaces.\n",
    "\n",
    "Removing stop words (very common words like ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù that don‚Äôt carry much meaning).\n",
    "\n",
    "Converting words to their base form using stemming or lemmatization (for example, ‚Äúrunning‚Äù, ‚Äúruns‚Äù, and ‚Äúran‚Äù become ‚Äúrun‚Äù).\n",
    "\n",
    "This helps reduce noise and makes the text more meaningful for the model.\n",
    "\n",
    "c) Converting Text to Numbers\n",
    "\n",
    "Machine learning models can‚Äôt directly understand text ‚Äî they need numbers.\n",
    "So we use a technique called vectorization.\n",
    "\n",
    "The most common and effective method for spam detection is TF-IDF (Term Frequency‚ÄìInverse Document Frequency).\n",
    "It measures how important a word is in an email relative to all other emails.\n",
    "For example, common words like ‚Äúthe‚Äù get low importance, while rare but meaningful words like ‚Äúlottery‚Äù or ‚Äúwinner‚Äù get higher importance.\n",
    "\n",
    "2. Choosing the Right Model\n",
    "\n",
    "Two popular algorithms are suitable here: Na√Øve Bayes and Support Vector Machine (SVM).\n",
    "\n",
    "Na√Øve Bayes\n",
    "\n",
    "Works extremely well for text classification.\n",
    "\n",
    "It assumes that all words are independent of each other, which isn‚Äôt always true, but in practice it performs surprisingly well.\n",
    "\n",
    "It‚Äôs fast, easy to train, and very effective for spam filtering.\n",
    "\n",
    "SVM (Support Vector Machine)\n",
    "\n",
    "Builds a boundary between ‚Äúspam‚Äù and ‚Äúnot spam‚Äù emails in a high-dimensional space.\n",
    "\n",
    "It‚Äôs powerful and can handle complex relationships, but it‚Äôs slower and needs parameter tuning.\n",
    "\n",
    "Decision: Start with Na√Øve Bayes because it‚Äôs quick, efficient, and well-suited for text data. Later, you can compare it with an SVM model to see if accuracy improves.\n",
    "\n",
    "3. Handling Class Imbalance\n",
    "\n",
    "In real-world data, the number of spam emails is usually much smaller than legitimate emails.\n",
    "If not handled properly, the model might just predict ‚ÄúNot Spam‚Äù for everything to achieve high accuracy ‚Äî but that‚Äôs not useful.\n",
    "\n",
    "To fix this, you can:\n",
    "\n",
    "Oversample the minority class (Spam): Duplicate or generate synthetic spam examples using techniques like SMOTE.\n",
    "\n",
    "Undersample the majority class (Not Spam): Randomly remove some non-spam emails to balance the dataset.\n",
    "\n",
    "Use class weights: Some models (like SVM) can automatically give more importance to the minority class by setting a higher penalty for misclassifying spam emails.\n",
    "\n",
    "Adjust probability threshold: Instead of using the default 50% cutoff for classification, lower the threshold so the model catches more spam (increasing recall).\n",
    "\n",
    "4. Evaluating the Model\n",
    "\n",
    "Since the dataset is imbalanced, accuracy alone is not a good metric ‚Äî it can be misleading.\n",
    "Instead, we focus on:\n",
    "\n",
    "Precision: How many of the emails predicted as spam are actually spam?\n",
    "(We want high precision to avoid marking important emails as spam.)\n",
    "\n",
    "Recall: How many of the actual spam emails were correctly identified?\n",
    "(We want high recall so that spam doesn‚Äôt slip through.)\n",
    "\n",
    "F1-Score: A balance between precision and recall.\n",
    "\n",
    "ROC-AUC: Measures how well the model can distinguish between spam and not spam overall.\n",
    "\n",
    "These metrics give a more complete picture of model performance.\n",
    "\n",
    "5. Business Impact\n",
    "\n",
    "Implementing this system has major benefits for the company:\n",
    "\n",
    "| Area                | Business Benefit                                                                  |\n",
    "| ------------------- | --------------------------------------------------------------------------------- |\n",
    "| **Security**        | Automatically blocks phishing, scams, and malware emails before they reach users. |\n",
    "| **Productivity**    | Employees don‚Äôt waste time sorting through junk emails.                           |\n",
    "| **User Experience** | Users trust the company‚Äôs email service more, improving reputation.               |\n",
    "| **Cost Efficiency** | Reduces server load, storage, and manual support efforts.                         |\n",
    "| **Compliance**      | Helps the company follow data protection and security policies.                   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
