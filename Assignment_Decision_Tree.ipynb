{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a6a6bc",
   "metadata": {},
   "source": [
    "Q1. What is a Decision Tree, and how does it work in the context of\n",
    "classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d89b8",
   "metadata": {},
   "source": [
    "A Decision Tree in Machine Learning is a supervised learning algorithm that makes predictions by splitting data into branches based on decision rules derived from the input features.It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
    "\n",
    "How it works in the context of classification:\n",
    "\n",
    "* Root Node: The process begins with the entire dataset at the root node.\n",
    "\n",
    "* Attribute Selection: The algorithm selects the \"best\" attribute to split the data based on a criterion like Gini impurity or information gain. This attribute is chosen because it best separates the data into distinct classes.\n",
    "\n",
    "* Splitting: The root node is then split into sub-nodes (branches) based on the different values or thresholds of the selected attribute. Each branch represents a possible outcome of the test on that attribute.\n",
    "\n",
    "* Recursive Partitioning: This splitting process is recursively applied to each sub-node, creating further branches and internal nodes. The goal is to create increasingly \"pure\" subsets of data, meaning subsets where most instances belong to the same class.\n",
    "\n",
    "* Leaf Nodes: The process continues until a stopping criterion is met, such as reaching a maximum tree depth, having a minimum number of samples in a node, or when a node becomes sufficiently pure (contains instances of predominantly one class). The final nodes in the tree are called leaf nodes, and each leaf node represents a class label.\n",
    "\n",
    "* Classification: To classify a new, unseen data point, it traverses the tree from the root node, following the branches corresponding to its attribute values until it reaches a leaf node. The class label associated with that leaf node is the predicted class for the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24208215",
   "metadata": {},
   "source": [
    "Q2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e569d",
   "metadata": {},
   "source": [
    "Gini Impurity and Entropy are both metrics for measuring the \"impurity\" or disorder of a node in a decision tree, with the goal of minimizing impurity at each split to create a pure, or well-classified, leaf node. Gini Impurity quantifies the probability of misclassification, favoring quicker, greedy splits towards the dominant class, while Entropy measures data uncertainty, preferring more balanced splits and yielding more information, though it is computationally more expensive. Decision trees use these criteria to find the best feature to split the data, selecting the split that results in the greatest reduction of impurity (also known as the most information gain). \n",
    "\n",
    "* Gini Impurity\n",
    "\n",
    "Concept: The probability of a randomly selected element being incorrectly classified if it were randomly labeled. \n",
    "\n",
    "Calculation: A node is considered pure (0% impurity) when all its data points belong to the same class. \n",
    "\n",
    "Effect on Splits: Gini impurity generally favors splits that isolate the most frequent class, resulting in faster, greedier splits. \n",
    "\n",
    "* Entropy\n",
    "\n",
    "Concept: A measure of the uncertainty or disorder in a node, reflecting the variety of class labels. \n",
    "\n",
    "Calculation: Uses a formula with logarithms (∑ -pᵢ * log₂(pᵢ)), where pᵢ is the probability of a specific outcome. \n",
    "\n",
    "Effect on Splits: Entropy provides a more nuanced measure of impurity and is more sensitive to the distribution of classes. It tends to result in more balanced splits and deeper trees. \n",
    "\n",
    "* Impact on Decision Tree Splits\n",
    "\n",
    "Objective: At each node, the decision tree evaluates different features and split points to find the one that minimizes impurity. \n",
    "\n",
    "Selection Process: The tree calculates the impurity of the resulting child nodes for each potential split. \n",
    "\n",
    "Information Gain: The best split is the one that leads to the largest reduction in impurity, also known as the maximum information gain. \n",
    "\n",
    "Criteria Choice: You can choose to use either \"gini\" or \"entropy\" as the splitting criterion in your decision tree model, with Gini often preferred for its faster computation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92ab78",
   "metadata": {},
   "source": [
    "Q3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
    "Trees? Give one practical advantage of using each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5af2d8",
   "metadata": {},
   "source": [
    "Pre-pruning halts a decision tree's growth during training by setting stopping criteria, while post-pruning removes unnecessary branches from a fully grown tree after it's built\n",
    "\n",
    "* Pre-Pruning (Early Stopping)\n",
    "\n",
    "What it is: This method prevents the decision tree from growing too large by setting criteria (like maximum depth, minimum samples per leaf) to stop the splitting process early \n",
    "during the tree construction. \n",
    "\n",
    "Practical Advantage: It's faster and more efficient, especially for large datasets, because it doesn't require building a massive, fully grown tree only to trim it later. \n",
    "\n",
    "* Post-Pruning (Backward Pruning)\n",
    "\n",
    "What it is: After the decision tree is fully grown, unnecessary subtrees or branches are removed to simplify the model. \n",
    "\n",
    "Practical Advantage: It can lead to better generalization and higher accuracy because it uses techniques like cross-validation to check if removing a branch improves the model's\n",
    "performance on unseen data, rather than making a decision without full information, as can happen with pre-pruning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4ce41",
   "metadata": {},
   "source": [
    "Q4.  What is Information Gain in Decision Trees, and why is it important for\n",
    "choosing the best split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3fef7",
   "metadata": {},
   "source": [
    "Information Gain measures the reduction in entropy (uncertainty) of the target variable after a feature is used to split the data, indicating how much information that feature provides about the class labels. It's important because it guides decision tree algorithms to select the best feature for splitting at each node, leading to more homogeneous child nodes and a more accurate, efficient, and predictive model. \n",
    "\n",
    "* Why it's Important for Choosing the Best Split\n",
    "\n",
    "Maximize Purity: The goal of a decision tree is to create nodes that are as pure as possible, meaning they contain instances of only one class. Information Gain helps achieve this by quantifying which feature, when used for splitting, results in the most significant reduction in impurity. \n",
    "\n",
    "Feature Importance: Features with higher Information Gain are more informative and thus more useful for classification. By choosing the feature with the highest Information Gain at each step, the algorithm prioritizes the most relevant features for making decisions, leading to a more effective tree structure. \n",
    "\n",
    "Build Predictive Accuracy: A decision tree built using Information Gain as the splitting criterion is more likely to have high predictive accuracy. This is because the best splits lead to clearer, more separated groups of data, which improves the overall decision-making process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0bd3a",
   "metadata": {},
   "source": [
    "Q5. What are some common real-world applications of Decision Trees, and\n",
    "what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e8e61",
   "metadata": {},
   "source": [
    "Decision Trees are used in various fields, including healthcare for diagnosis, finance for risk assessment and fraud detection, marketing for customer segmentation and churn prediction, and education to identify at-risk students. \n",
    "\n",
    "* Real-World Applications\n",
    "\n",
    "Healthcare: Used for disease diagnosis by analyzing patient symptoms and data, and for determining personalized treatment plans. \n",
    "\n",
    "Finance: Applied for loan application approval, credit scoring, and detecting fraudulent transactions. \n",
    "\n",
    "Marketing: Employed to segment customers, predict customer behavior like churn, and personalize marketing campaigns. \n",
    "\n",
    "Business: Aids in strategic planning, resource allocation, and inventory management by forecasting sales. \n",
    "\n",
    "Education: Helps predict student performance (pass/fail) based on factors like attendance and grades to provide targeted support. \n",
    "\n",
    "Manufacturing: Optimizes production processes by analyzing efficiency factors and predicting trends. \n",
    "\n",
    "* Advantages\n",
    "\n",
    "Interpretability: Decision trees are easy to understand, visualize, and explain to non-technical audiences due to their tree-like structure. \n",
    "\n",
    "Minimal Data Preparation: They require less data preprocessing and cleaning compared to other algorithms, making them quicker to implement. \n",
    "\n",
    "Handle Mixed Data Types: Decision trees can handle both numerical and categorical data, providing flexibility in data input. \n",
    "\n",
    "Versatility: They can be used for both classification (e.g., diagnosing a disease) and regression (e.g., predicting house prices). \n",
    "\n",
    "Feature Importance: The structure of the tree can indicate which features are most important in making a prediction, aiding in feature selection. \n",
    "\n",
    "* Limitations \n",
    "\n",
    "Prone to Overfitting: Decision trees can become too complex, leading to overfitting where they perform well on training data but poorly on new data.\n",
    "\n",
    "Unstable to Data Changes: Small changes in the data or the introduction of noise can lead to very different tree structures and predictions.\n",
    "\n",
    "Non-Continuous Predictions: For regression tasks, decision trees produce piecewise constant predictions, not smooth, continuous ones.\n",
    "\n",
    "Computational Complexity: For large datasets, building and interpreting decision trees can be computationally expensive and complex.\n",
    "\n",
    "Greedy Approach: The algorithm makes locally optimal decisions at each step, which may not lead to a globally optimal solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af0c4e",
   "metadata": {},
   "source": [
    "Q6. Write a Python program to:\n",
    "\n",
    "● Load the Iris Dataset\n",
    "\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "\n",
    "● Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ae7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba63a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1cf4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0eb1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264086e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bd19865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Decision Tree Classifier using Gini criterion\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "clf\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b60100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test data set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddf6e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Accuracy is 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The Model Accuracy is\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736ae83",
   "metadata": {},
   "source": [
    "Q7. Write a Python program to:\n",
    "\n",
    "● Load the Iris Dataset\n",
    "\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04958092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1ad8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce0e5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef4b62a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the data a fully grown decision Tree\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full\n",
    "\n",
    "clf_full.fit(X_train, y_train)\n",
    "\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "accuracy_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afdfa5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Decision Tree with max_depth=3\n",
    "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pruned = clf_pruned.predict(X_test)\n",
    "\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "accuracy_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2d746",
   "metadata": {},
   "source": [
    "Q8. Write a Python program to:\n",
    "\n",
    "● Load the California Housing dataset from sklearn\n",
    "\n",
    "● Train a Decision Tree Regressor\n",
    "\n",
    "● Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "291dfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21a6d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d875f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(random_state=42)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Decision Tree Regressor\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4299d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ecfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5280096503174904"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9bcb10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.523\n",
      "HouseAge: 0.052\n",
      "AveRooms: 0.049\n",
      "AveBedrms: 0.025\n",
      "Population: 0.032\n",
      "AveOccup: 0.139\n",
      "Latitude: 0.090\n",
      "Longitude: 0.089\n"
     ]
    }
   ],
   "source": [
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(data.feature_names, reg.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18979f29",
   "metadata": {},
   "source": [
    "Q9. Write a Python program to:\n",
    "\n",
    "● Load the Iris Dataset\n",
    "\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "\n",
    "● Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f327b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Model Accuracy on Test Set: 0.978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Define the Decision Tree and hyperparameter grid\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [2, 3, 4, 5, None],         # Possible depths\n",
    "    \"min_samples_split\": [2, 3, 4, 5, 10]    # Minimum samples required to split a node\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
    "                           cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(f\"Model Accuracy on Test Set: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9eada1",
   "metadata": {},
   "source": [
    "Q10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "\n",
    "Explain the step-by-step process you would follow to:\n",
    "\n",
    "● Handle the missing values\n",
    "\n",
    "● Encode the categorical features\n",
    "\n",
    "● Train a Decision Tree model\n",
    "\n",
    "● Tune its hyperparameters\n",
    "\n",
    "● Evaluate its performance\n",
    "\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e0d02",
   "metadata": {},
   "source": [
    "Step-by-Step Process\n",
    "\n",
    "Handle Missing Values:\n",
    "\n",
    "Identify Missing Data: Understand the extent and nature of missing data by examining the dataset.\n",
    "\n",
    "Choose an Imputation Strategy:\n",
    "\n",
    "For numerical features: Use methods like mean imputation, median imputation, or predictive imputation (e.g., using another machine learning model to predict the missing value). \n",
    "\n",
    "For categorical features: Impute with the most frequent category (mode) or create a new category for missing values. \n",
    "\n",
    "Consider Deletion: If a large percentage of data is missing for a particular feature, or if the missing data is random, consider dropping the rows or columns, but be cautious as this can lead to loss of valuable information. \n",
    "\n",
    "Encode Categorical Features:\n",
    "\n",
    "Understand Categorical Features: Identify features with text or non-numeric values (e.g., 'gender', 'symptoms'). \n",
    "\n",
    "Choose an Encoding Method:\n",
    "\n",
    "One-Hot Encoding: For nominal categorical features (no inherent order), create binary columns for each category. \n",
    "\n",
    "Label Encoding: For ordinal categorical features (with a clear order), assign numerical labels to each category. \n",
    "\n",
    "Binary Encoding: For categories with many unique values, this method is more efficient than one-hot encoding. \n",
    "\n",
    "Train a Decision Tree Model:\n",
    "\n",
    "Split the Data: Divide the preprocessed dataset into training and testing sets to ensure the model can generalize to new data. \n",
    "\n",
    "Instantiate the Model: Create a Decision Tree classifier using a library like scikit-learn. \n",
    "\n",
    "Fit the Model: Train the Decision Tree on the training data to learn the patterns that differentiate diseased from healthy patients. \n",
    "\n",
    "Tune its Hyperparameters:\n",
    "\n",
    "Define Hyperparameters: Identify key Decision Tree parameters that can be adjusted, such as max_depth, min_samples_split, and min_samples_leaf. \n",
    "\n",
    "Use Cross-Validation: Employ k-fold cross-validation to evaluate different hyperparameter combinations without overfitting to the training data. \n",
    "\n",
    "Apply Tuning Techniques:\n",
    "\n",
    "GridSearchCV: Systematically try every combination of hyperparameters from a specified grid. \n",
    "\n",
    "RandomizedSearchCV: Randomly sample hyperparameter combinations, which can be more efficient for a large search space. \n",
    "\n",
    "Evaluate its Performance:\n",
    "\n",
    "Use Appropriate Metrics: Evaluate the tuned model's performance on the unseen test data. \n",
    "\n",
    "Common Metrics for Classification:\n",
    "\n",
    "Accuracy: Overall percentage of correct predictions. \n",
    "\n",
    "Precision: Of all patients predicted to have the disease, what proportion actually have it. \n",
    "\n",
    "Recall (Sensitivity): Of all patients who actually have the disease, what proportion were correctly identified. \n",
    "\n",
    "F1-Score: The harmonic mean of precision and recall, balancing both. \n",
    "\n",
    "AUC-ROC Curve: Measures the model's ability to distinguish between classes. \n",
    "\n",
    "Business Value\n",
    "\n",
    "This predictive model can provide significant business value to a healthcare company by: \n",
    "\n",
    "Early Diagnosis: Enabling earlier identification of diseases, leading to more timely interventions and better patient outcomes.\n",
    "\n",
    "Personalized Treatment: Assisting in tailoring treatment plans based on a patient's specific risk factors and disease presentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
